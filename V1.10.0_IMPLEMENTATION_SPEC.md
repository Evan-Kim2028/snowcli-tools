# v1.10.0 Discovery Assistant - Implementation Specification

**Version**: 1.10.0
**Branch**: `v1.10.0_discovery_assistant`
**Status**: Ready for Implementation
**Date**: 2025-01-05
**Based on**: Comprehensive planning in `notes/1.10.0_discovery_assistant/`

---

## Executive Summary

v1.10.0 introduces **Discovery Assistant** - an AI-powered MCP tool that automatically profiles, analyzes, and documents poorly-documented Snowflake tables using Cortex Complete AI inference.

**Scope**: Discovery Assistant ONLY (Audit logging deferred to v1.11.0)

**Key Decision**: Phased approach to maintain code quality (7.5/10) and reduce complexity

---

## Architecture Overview

### Component Structure

```
src/snowcli_tools/
├── discovery/                          # NEW - Discovery Assistant module
│   ├── __init__.py                    # Public API exports
│   ├── models.py                      # Data models (TableProfile, LLMAnalysis, etc.)
│   ├── profiler.py                    # TableProfiler - SQL-based profiling
│   ├── llm_analyzer.py                # LLMAnalyzer - Cortex Complete inference
│   ├── relationship_discoverer.py     # RelationshipDiscoverer - FK detection
│   ├── documentation_generator.py     # DocumentationGenerator - Output formatting
│   └── utils.py                       # Shared utilities (PII detection, etc.)
│
├── mcp/
│   └── tools/
│       └── discover_table_purpose.py  # NEW - MCP tool implementation
│
└── tests/
    └── discovery/                      # NEW - Discovery tests
        ├── test_profiler.py           # TableProfiler tests (20 tests)
        ├── test_llm_analyzer.py       # LLMAnalyzer tests (15 tests)
        ├── test_relationships.py      # RelationshipDiscoverer tests (15 tests)
        ├── test_documentation.py      # DocumentationGenerator tests (10 tests)
        ├── test_mcp_tool.py           # MCP tool tests (20 tests)
        ├── test_batch_discovery.py    # Batch discovery tests (8 tests)
        └── test_integration.py        # End-to-end tests (5 tests)
```

### Data Flow

```
Agent Request → discover_table_purpose()
                      ↓
              [Depth Router]
                      ↓
         ┌────────────┴────────────┐
         │                         │
    [TableProfiler]          [Cache Check]
         │                         │
         ├──> Column stats         │
         ├──> Pattern detection    │
         └──> Sample data          │
                      ↓
              [LLMAnalyzer]
         (if depth ≥ standard)
                      ↓
         ├──> Business purpose
         ├──> Column meanings
         └──> PII detection
                      ↓
         [RelationshipDiscoverer]
          (if depth = deep)
                      ↓
         ├──> Name patterns
         └──> Value overlap
                      ↓
         [DocumentationGenerator]
                      ↓
         ├──> Markdown report
         └──> JSON export
                      ↓
              [Cache Store]
                      ↓
         ← DiscoveryResult (return to agent)
```

---

## Data Models

### File: `src/snowcli_tools/discovery/models.py`

```python
"""
Data models for Discovery Assistant.

This module defines all data structures used by the Discovery Assistant,
following Pydantic for validation and serialization.
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Optional


class DepthMode(str, Enum):
    """Discovery depth modes."""
    QUICK = "quick"        # SQL profiling only
    STANDARD = "standard"  # + LLM analysis
    DEEP = "deep"          # + Relationship discovery


class OutputFormat(str, Enum):
    """Output format options."""
    MARKDOWN = "markdown"
    JSON = "json"


class CachePolicy(str, Enum):
    """Cache behavior policies."""
    ALWAYS = "always"      # Always use cache if available
    IF_FRESH = "if_fresh"  # Use cache if table hasn't changed (LAST_DDL check)
    NEVER = "never"        # Always re-discover


class ConfidenceLevel(str, Enum):
    """Confidence interpretation levels."""
    HIGH = "high"      # 80-100%
    MEDIUM = "medium"  # 60-79%
    LOW = "low"        # <60%


class TableCategory(str, Enum):
    """Table category classifications."""
    DIMENSION = "dimension_table"
    FACT = "fact_table"
    EVENT_LOG = "event_log"
    REFERENCE = "reference_data"
    UNKNOWN = "unknown"


@dataclass
class ColumnProfile:
    """Statistical profile for a single column."""
    name: str
    data_type: str
    null_percentage: float
    cardinality: int
    pattern: Optional[str] = None  # "email", "uuid", "phone", "url", etc.
    sample_values: list[Any] = field(default_factory=list)
    min_value: Optional[Any] = None
    max_value: Optional[Any] = None
    avg_length: Optional[float] = None


@dataclass
class TableProfile:
    """Complete statistical profile of a table."""
    database: str
    schema: str
    table_name: str
    row_count: int
    columns: list[ColumnProfile]
    sample_rows: list[dict[str, Any]]
    profiling_time_seconds: float
    last_ddl: Optional[str] = None  # ISO timestamp


@dataclass
class ColumnMeaning:
    """AI-inferred meaning of a column."""
    purpose: str
    category: str  # "id", "name", "contact", "timestamp", etc.
    is_pii: bool
    confidence: float


@dataclass
class LLMAnalysis:
    """AI inference results from Cortex Complete."""
    table_purpose: str
    category: TableCategory
    confidence: float  # 0.0-1.0
    column_meanings: dict[str, ColumnMeaning]
    pii_columns: list[str]
    suggested_description: str
    analysis_time_seconds: float
    token_usage: Optional[dict[str, int]] = None  # {"prompt": X, "response": Y}


@dataclass
class Relationship:
    """Discovered foreign key relationship."""
    from_table: str
    from_column: str
    to_table: str
    to_column: str
    confidence: float
    evidence: list[str]
    strategy: str  # "name_pattern", "value_overlap", "combined"


@dataclass
class DiscoveryMetadata:
    """Enhanced metadata for discovery results."""
    execution_time_ms: int
    estimated_cost_usd: float
    cache_hit: bool
    table_last_modified: str  # ISO timestamp
    discovery_version: str  # "1.10.0"
    confidence_level: ConfidenceLevel
    depth_mode: DepthMode
    timestamp: str  # ISO timestamp of discovery


@dataclass
class DiscoveryResult:
    """Complete discovery output."""
    profile: TableProfile
    analysis: Optional[LLMAnalysis] = None  # None for quick mode
    relationships: list[Relationship] = field(default_factory=list)
    documentation: str = ""  # Markdown or JSON formatted
    metadata: DiscoveryMetadata = None

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        # Implementation provided in class
        pass
```

**Estimated LOC**: 150-200

---

## Milestone 1: TableProfiler Implementation

### File: `src/snowcli_tools/discovery/profiler.py`

**Purpose**: SQL-based table profiling without external dependencies

**Key Requirements**:
- ✅ No pandas (pure SQL)
- ✅ <5s for 1M+ row tables
- ✅ Pattern detection (email, UUID, phone, URL)
- ✅ SQL injection prevention (identifier quoting)
- ✅ **CRITICAL FIX**: No CROSS JOIN

**Implementation**:

```python
"""
TableProfiler - SQL-based table profiling.

Profiles Snowflake tables using pure SQL queries to extract:
- Column statistics (null%, cardinality, min/max, avg_length)
- Pattern detection (email, UUID, phone, URL, dates)
- Sample data (configurable limit)
- Large table optimizations (sampling, APPROX_COUNT_DISTINCT)
"""

import re
from typing import Optional
from datetime import datetime

from snowcli_tools.discovery.models import TableProfile, ColumnProfile
from snowcli_tools.snow_cli import SnowCLI


class TableProfiler:
    """Profile Snowflake tables using SQL queries."""

    # Pattern detection regexes
    PATTERNS = {
        "email": r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$",
        "uuid": r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$",
        "phone": r"^\+?[\d\s\-\(\)]{10,}$",
        "url": r"^https?://[^\s]+$",
    }

    def __init__(
        self,
        snow_cli: SnowCLI,
        sample_size: int = 100,
        use_approx_count: bool = True,
        timeout_seconds: int = 30
    ):
        """
        Initialize TableProfiler.

        Args:
            snow_cli: SnowCLI instance for executing queries
            sample_size: Number of sample rows to extract (default: 100)
            use_approx_count: Use APPROX_COUNT_DISTINCT for large tables (default: True)
            timeout_seconds: Query timeout in seconds (default: 30)
        """
        self.snow_cli = snow_cli
        self.sample_size = sample_size
        self.use_approx_count = use_approx_count
        self.timeout_seconds = timeout_seconds

    def profile_table(
        self,
        table_name: str,
        database: Optional[str] = None,
        schema: Optional[str] = None
    ) -> TableProfile:
        """
        Profile a Snowflake table.

        Args:
            table_name: Table name
            database: Database name (optional, uses session default)
            schema: Schema name (optional, uses session default)

        Returns:
            TableProfile with complete statistical analysis

        Raises:
            ValueError: If table doesn't exist or invalid identifiers
        """
        start_time = datetime.now()

        # Parse and quote table identifiers (SQL injection prevention)
        db, sch, tbl = self._parse_and_quote(table_name, database, schema)
        full_table = f'"{db}"."{sch}"."{tbl}"'

        # Get table metadata
        row_count = self._get_row_count(full_table)
        last_ddl = self._get_last_ddl(db, sch, tbl)

        # Get column information from INFORMATION_SCHEMA
        columns_info = self._get_columns_info(db, sch, tbl)

        # Profile each column (CRITICAL: avoid CROSS JOIN)
        column_profiles = self._profile_columns(full_table, columns_info, row_count)

        # Get sample rows
        sample_rows = self._get_sample_rows(full_table)

        end_time = datetime.now()
        profiling_time = (end_time - start_time).total_seconds()

        return TableProfile(
            database=db,
            schema=sch,
            table_name=tbl,
            row_count=row_count,
            columns=column_profiles,
            sample_rows=sample_rows,
            profiling_time_seconds=profiling_time,
            last_ddl=last_ddl
        )

    def _parse_and_quote(
        self,
        table_name: str,
        database: Optional[str],
        schema: Optional[str]
    ) -> tuple[str, str, str]:
        """
        Parse table name and return quoted identifiers.

        SECURITY: Prevents SQL injection by properly quoting identifiers.
        """
        from snowcli_tools.snow_cli import parse_table_name

        # Use existing utility to parse
        db, sch, tbl = parse_table_name(table_name)

        # Override with explicit parameters if provided
        if database:
            db = database
        if schema:
            sch = schema

        # Validate identifiers (basic sanity check)
        for identifier in [db, sch, tbl]:
            if not identifier or not re.match(r'^[A-Za-z0-9_]+$', identifier):
                raise ValueError(f"Invalid identifier: {identifier}")

        return db, sch, tbl

    def _get_row_count(self, full_table: str) -> int:
        """Get approximate row count."""
        sql = f"SELECT COUNT(*) as cnt FROM {full_table}"
        result = self.snow_cli.execute_query(sql)
        return int(result[0]["cnt"])

    def _get_last_ddl(self, database: str, schema: str, table: str) -> Optional[str]:
        """Get LAST_DDL timestamp from INFORMATION_SCHEMA."""
        sql = f"""
            SELECT LAST_DDL
            FROM "{database}".INFORMATION_SCHEMA.TABLES
            WHERE TABLE_SCHEMA = '{schema}'
              AND TABLE_NAME = '{table}'
        """
        result = self.snow_cli.execute_query(sql)
        if result and result[0].get("LAST_DDL"):
            return result[0]["LAST_DDL"].isoformat()
        return None

    def _get_columns_info(self, database: str, schema: str, table: str) -> list[dict]:
        """Get column metadata from INFORMATION_SCHEMA."""
        sql = f"""
            SELECT
                COLUMN_NAME,
                DATA_TYPE,
                ORDINAL_POSITION
            FROM "{database}".INFORMATION_SCHEMA.COLUMNS
            WHERE TABLE_SCHEMA = '{schema}'
              AND TABLE_NAME = '{table}'
            ORDER BY ORDINAL_POSITION
        """
        return self.snow_cli.execute_query(sql)

    def _profile_columns(
        self,
        full_table: str,
        columns_info: list[dict],
        row_count: int
    ) -> list[ColumnProfile]:
        """
        Profile all columns efficiently.

        CRITICAL FIX: Use per-column queries with UNION instead of CROSS JOIN.

        Original (WRONG):
            SELECT column_name, COUNT(*)
            FROM INFORMATION_SCHEMA.COLUMNS c
            CROSS JOIN {table}  # Creates cartesian product!

        Fixed (CORRECT):
            Use UNION of per-column queries
        """
        column_profiles = []

        # Build UNION query for all columns
        queries = []
        for col_info in columns_info:
            col_name = col_info["COLUMN_NAME"]
            data_type = col_info["DATA_TYPE"]

            # Use APPROX_COUNT_DISTINCT for large tables
            count_fn = "APPROX_COUNT_DISTINCT" if self.use_approx_count and row_count > 100_000 else "COUNT(DISTINCT"

            query = f"""
                SELECT
                    '{col_name}' as column_name,
                    '{data_type}' as data_type,
                    COUNT(*) as total_rows,
                    COUNT("{col_name}") as non_null_count,
                    {count_fn}("{col_name}") as cardinality,
                    MIN(LENGTH("{col_name}"::VARCHAR)) as min_len,
                    MAX(LENGTH("{col_name}"::VARCHAR)) as max_len,
                    AVG(LENGTH("{col_name}"::VARCHAR)) as avg_len
                FROM {full_table}
            """
            queries.append(query)

        # Execute combined query
        union_sql = " UNION ALL ".join(queries)
        results = self.snow_cli.execute_query(union_sql)

        # Get sample values for pattern detection
        sample_sql = f"SELECT * FROM {full_table} LIMIT {min(self.sample_size, 100)}"
        sample_data = self.snow_cli.execute_query(sample_sql)

        # Build ColumnProfile objects
        for i, result in enumerate(results):
            col_name = result["COLUMN_NAME"]
            data_type = result["DATA_TYPE"]
            total_rows = int(result["TOTAL_ROWS"])
            non_null = int(result["NON_NULL_COUNT"])
            cardinality = int(result["CARDINALITY"])
            avg_len = float(result["AVG_LEN"]) if result["AVG_LEN"] else None

            # Calculate null percentage
            null_pct = ((total_rows - non_null) / total_rows * 100) if total_rows > 0 else 0

            # Extract sample values from sample data
            sample_values = [row.get(col_name) for row in sample_data if row.get(col_name) is not None][:10]

            # Detect patterns
            pattern = self._detect_pattern(sample_values, data_type)

            column_profiles.append(ColumnProfile(
                name=col_name,
                data_type=data_type,
                null_percentage=null_pct,
                cardinality=cardinality,
                pattern=pattern,
                sample_values=sample_values,
                avg_length=avg_len
            ))

        return column_profiles

    def _detect_pattern(self, sample_values: list[Any], data_type: str) -> Optional[str]:
        """
        Detect common patterns in column values.

        Returns:
            Pattern name ("email", "uuid", "phone", "url") or None
        """
        if not sample_values or data_type not in ["VARCHAR", "TEXT", "STRING"]:
            return None

        # Convert to strings
        str_values = [str(v) for v in sample_values if v is not None]
        if not str_values:
            return None

        # Check each pattern
        for pattern_name, pattern_regex in self.PATTERNS.items():
            matches = sum(1 for v in str_values if re.match(pattern_regex, v, re.IGNORECASE))
            match_rate = matches / len(str_values)

            # If 80%+ match, consider it that pattern
            if match_rate >= 0.8:
                return pattern_name

        return None

    def _get_sample_rows(self, full_table: str) -> list[dict[str, Any]]:
        """Get sample rows from table."""
        sql = f"SELECT * FROM {full_table} LIMIT {self.sample_size}"
        return self.snow_cli.execute_query(sql)
```

**Estimated LOC**: 400-500

### Tests: `tests/discovery/test_profiler.py`

**Test Coverage** (20 tests, 90%+ coverage):

```python
"""Tests for TableProfiler."""

import pytest
from snowcli_tools.discovery.profiler import TableProfiler
from snowcli_tools.discovery.models import TableProfile, ColumnProfile


class TestTableProfiler:
    """Test suite for TableProfiler."""

    # Basic functionality
    def test_profile_table_basic(self, mock_snow_cli):
        """Test basic table profiling."""
        pass

    def test_profile_table_with_explicit_database_schema(self, mock_snow_cli):
        """Test profiling with explicit database and schema."""
        pass

    # Pattern detection
    def test_detect_email_pattern(self, mock_snow_cli):
        """Test email pattern detection (95%+ accuracy)."""
        pass

    def test_detect_uuid_pattern(self, mock_snow_cli):
        """Test UUID pattern detection."""
        pass

    def test_detect_phone_pattern(self, mock_snow_cli):
        """Test phone number pattern detection."""
        pass

    def test_detect_url_pattern(self, mock_snow_cli):
        """Test URL pattern detection."""
        pass

    def test_no_pattern_for_non_string_columns(self, mock_snow_cli):
        """Test that pattern detection skips non-string columns."""
        pass

    # Performance & scalability
    def test_large_table_uses_approx_count(self, mock_snow_cli):
        """Test APPROX_COUNT_DISTINCT for tables >100K rows."""
        pass

    def test_performance_1m_rows_under_5_seconds(self, mock_snow_cli):
        """CRITICAL: Verify <5s for 1M row table."""
        pass

    # SQL injection prevention
    def test_sql_injection_prevention_table_name(self, mock_snow_cli):
        """Test that SQL injection is prevented via identifier quoting."""
        pass

    def test_invalid_identifier_raises_error(self, mock_snow_cli):
        """Test that invalid identifiers are rejected."""
        pass

    # Edge cases
    def test_empty_table(self, mock_snow_cli):
        """Test profiling empty table (0 rows)."""
        pass

    def test_single_column_table(self, mock_snow_cli):
        """Test profiling table with single column."""
        pass

    def test_table_with_1000_columns(self, mock_snow_cli):
        """Test profiling wide table (1000+ columns)."""
        pass

    def test_table_with_variant_columns(self, mock_snow_cli):
        """Test handling of VARIANT/ARRAY/OBJECT columns."""
        pass

    def test_table_with_all_null_column(self, mock_snow_cli):
        """Test handling of columns with 100% null values."""
        pass

    # Error handling
    def test_table_not_found_raises_error(self, mock_snow_cli):
        """Test that non-existent table raises appropriate error."""
        pass

    def test_permission_denied_raises_error(self, mock_snow_cli):
        """Test handling of permission denied errors."""
        pass

    # Sample data
    def test_sample_rows_respects_limit(self, mock_snow_cli):
        """Test that sample rows respects configured limit."""
        pass

    def test_last_ddl_extraction(self, mock_snow_cli):
        """Test extraction of LAST_DDL timestamp."""
        pass
```

**Estimated LOC**: 600-800 (tests)

---

## Milestone 2: LLMAnalyzer Implementation

### File: `src/snowcli_tools/discovery/llm_analyzer.py`

**Purpose**: Use Cortex Complete to infer business purpose and meaning

**Key Requirements**:
- ✅ 75-85% accuracy on table purpose
- ✅ 95%+ PII detection
- ✅ Confidence scoring
- ✅ Error handling (malformed JSON, timeouts, hallucinations)

**Implementation**:

```python
"""
LLMAnalyzer - AI-powered business purpose inference.

Uses Snowflake Cortex Complete (mistral-large) to infer:
- Table business purpose
- Column meanings
- PII detection
- Category classification (dimension, fact, event log, reference)
"""

import json
import re
from typing import Optional
from datetime import datetime

from snowcli_tools.discovery.models import (
    TableProfile,
    LLMAnalysis,
    ColumnMeaning,
    TableCategory,
    ConfidenceLevel
)
from snowcli_tools.snow_cli import SnowCLI


class LLMAnalyzer:
    """Analyze table purpose using Cortex Complete AI."""

    DEFAULT_MODEL = "mistral-large"
    DEFAULT_TEMPERATURE = 0.1  # Low for consistency
    DEFAULT_MAX_TOKENS = 2048

    def __init__(
        self,
        snow_cli: SnowCLI,
        model: str = DEFAULT_MODEL,
        temperature: float = DEFAULT_TEMPERATURE,
        max_tokens: int = DEFAULT_MAX_TOKENS
    ):
        """
        Initialize LLMAnalyzer.

        Args:
            snow_cli: SnowCLI instance for Cortex Complete calls
            model: Cortex model to use (default: mistral-large)
            temperature: Sampling temperature (default: 0.1 for consistency)
            max_tokens: Maximum response tokens (default: 2048)
        """
        self.snow_cli = snow_cli
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens

    def analyze_table(self, profile: TableProfile) -> LLMAnalysis:
        """
        Analyze table using AI inference.

        Args:
            profile: TableProfile from TableProfiler

        Returns:
            LLMAnalysis with inferred business purpose, column meanings, PII detection

        Raises:
            ValueError: If LLM returns malformed response
            TimeoutError: If Cortex Complete call times out
        """
        start_time = datetime.now()

        # Build prompt
        prompt = self._build_analysis_prompt(profile)

        # Call Cortex Complete
        try:
            response = self._call_cortex_complete(prompt)

            # Parse JSON response
            analysis_data = self._parse_llm_response(response)

            # Calculate confidence level
            confidence = analysis_data.get("confidence", 0) / 100.0  # Convert to 0-1
            confidence_level = self._calculate_confidence_level(confidence)

            # Build ColumnMeaning objects
            column_meanings = {}
            for col_name, col_data in analysis_data.get("columns", {}).items():
                column_meanings[col_name] = ColumnMeaning(
                    purpose=col_data.get("purpose", "Unknown"),
                    category=col_data.get("category", "unknown"),
                    is_pii=col_data.get("is_pii", False),
                    confidence=col_data.get("confidence", 0) / 100.0
                )

            # Extract PII columns
            pii_columns = [
                col_name for col_name, meaning in column_meanings.items()
                if meaning.is_pii
            ]

            end_time = datetime.now()
            analysis_time = (end_time - start_time).total_seconds()

            return LLMAnalysis(
                table_purpose=analysis_data.get("table_purpose", "Unknown"),
                category=TableCategory(analysis_data.get("category", "unknown")),
                confidence=confidence,
                column_meanings=column_meanings,
                pii_columns=pii_columns,
                suggested_description=analysis_data.get("suggested_description", ""),
                analysis_time_seconds=analysis_time,
                token_usage=analysis_data.get("token_usage")
            )

        except json.JSONDecodeError as e:
            # LLM returned malformed JSON - fall back to low-confidence analysis
            return self._create_fallback_analysis(profile, str(e))
        except Exception as e:
            # Other errors - re-raise
            raise

    def _build_analysis_prompt(self, profile: TableProfile) -> str:
        """
        Build structured prompt for Cortex Complete.

        Prompt engineering for consistent, parseable responses.
        """
        # Format columns for prompt
        columns_text = "\n".join([
            f"  - {col.name} ({col.data_type}): "
            f"null={col.null_percentage:.1f}%, "
            f"cardinality={col.cardinality:,}, "
            f"pattern={col.pattern or 'none'}"
            for col in profile.columns
        ])

        # Format sample rows (first 3 only)
        sample_rows = profile.sample_rows[:3]
        sample_text = json.dumps(sample_rows, indent=2, default=str)

        prompt = f"""Analyze this Snowflake table and infer its business purpose.

TABLE: {profile.database}.{profile.schema}.{profile.table_name}
ROWS: {profile.row_count:,}

COLUMNS ({len(profile.columns)}):
{columns_text}

SAMPLE ROWS (first 3):
{sample_text}

Based on the table structure and sample data, provide a detailed analysis as JSON.

Respond with ONLY valid JSON (no markdown, no code blocks), using this exact structure:
{{
  "table_purpose": "Clear business description of what this table represents",
  "category": "dimension_table | fact_table | event_log | reference_data | unknown",
  "confidence": 85,
  "suggested_description": "2-3 sentence documentation for data catalog",
  "columns": {{
    "column_name": {{
      "purpose": "Business meaning of this column",
      "category": "id | name | contact | timestamp | amount | status | other",
      "is_pii": true,
      "confidence": 90
    }}
  }},
  "reasoning": "Brief explanation of your analysis"
}}

Key guidelines:
1. Set is_pii=true for ANY column that could identify an individual (email, phone, name, SSN, etc.)
2. Confidence should be 0-100 based on how certain you are
3. Use actual column names from the table in the "columns" object
4. Be specific and concise in purposes and descriptions

JSON response:"""

        return prompt

    def _call_cortex_complete(self, prompt: str) -> str:
        """
        Call Snowflake Cortex Complete.

        Uses Cortex Complete via SQL:
        SELECT SNOWFLAKE.CORTEX.COMPLETE(model, prompt, options)
        """
        # Build Cortex Complete SQL
        options = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        options_json = json.dumps(options)

        # Escape single quotes in prompt
        prompt_escaped = prompt.replace("'", "''")

        sql = f"""
            SELECT SNOWFLAKE.CORTEX.COMPLETE(
                '{self.model}',
                '{prompt_escaped}',
                {options_json}
            ) as response
        """

        result = self.snow_cli.execute_query(sql)

        if not result or not result[0].get("RESPONSE"):
            raise ValueError("Cortex Complete returned empty response")

        return result[0]["RESPONSE"]

    def _parse_llm_response(self, response: str) -> dict:
        """
        Parse LLM JSON response.

        Handles:
        - Markdown code blocks (```json ... ```)
        - Malformed JSON
        - Missing fields
        """
        # Strip markdown code blocks if present
        response = response.strip()
        if response.startswith("```"):
            # Extract JSON from code block
            match = re.search(r'```(?:json)?\s*(\{.*\})\s*```', response, re.DOTALL)
            if match:
                response = match.group(1)

        # Parse JSON
        try:
            data = json.loads(response)
        except json.JSONDecodeError:
            # Try to extract JSON object with regex
            match = re.search(r'\{.*\}', response, re.DOTALL)
            if match:
                data = json.loads(match.group(0))
            else:
                raise ValueError(f"Could not parse JSON from response: {response[:200]}")

        # Validate required fields
        required = ["table_purpose", "category", "confidence"]
        for field in required:
            if field not in data:
                raise ValueError(f"Missing required field: {field}")

        return data

    def _calculate_confidence_level(self, confidence: float) -> ConfidenceLevel:
        """Map confidence score to level."""
        if confidence >= 0.8:
            return ConfidenceLevel.HIGH
        elif confidence >= 0.6:
            return ConfidenceLevel.MEDIUM
        else:
            return ConfidenceLevel.LOW

    def _create_fallback_analysis(self, profile: TableProfile, error: str) -> LLMAnalysis:
        """
        Create low-confidence fallback analysis when LLM fails.

        Returns basic analysis based on table/column names only.
        """
        # Simple heuristic: look for common patterns in table name
        table_lower = profile.table_name.lower()

        if any(word in table_lower for word in ["customer", "user", "person"]):
            category = TableCategory.DIMENSION
            purpose = f"Likely a {profile.table_name} dimension table"
        elif any(word in table_lower for word in ["transaction", "order", "sale"]):
            category = TableCategory.FACT
            purpose = f"Likely a {profile.table_name} fact table"
        elif any(word in table_lower for word in ["event", "log", "audit"]):
            category = TableCategory.EVENT_LOG
            purpose = f"Likely a {profile.table_name} event log"
        else:
            category = TableCategory.UNKNOWN
            purpose = f"Table: {profile.table_name}"

        # Detect PII columns by name
        pii_keywords = ["email", "phone", "ssn", "name", "address"]
        pii_columns = [
            col.name for col in profile.columns
            if any(keyword in col.name.lower() for keyword in pii_keywords)
        ]

        return LLMAnalysis(
            table_purpose=purpose,
            category=category,
            confidence=0.3,  # Low confidence
            column_meanings={},
            pii_columns=pii_columns,
            suggested_description=f"Analysis failed: {error}. Using fallback heuristics.",
            analysis_time_seconds=0.0,
            token_usage=None
        )
```

**Estimated LOC**: 300-400

### Tests: `tests/discovery/test_llm_analyzer.py`

**Test Coverage** (15 tests):

```python
"""Tests for LLMAnalyzer."""

import pytest
from snowcli_tools.discovery.llm_analyzer import LLMAnalyzer


class TestLLMAnalyzer:
    """Test suite for LLMAnalyzer."""

    # Core functionality
    def test_analyze_table_dimension(self, mock_snow_cli, sample_customer_profile):
        """Test analysis of dimension table."""
        pass

    def test_analyze_table_fact(self, mock_snow_cli, sample_transaction_profile):
        """Test analysis of fact table."""
        pass

    def test_analyze_table_event_log(self, mock_snow_cli, sample_event_profile):
        """Test analysis of event log table."""
        pass

    # Prompt engineering
    def test_build_analysis_prompt_structure(self, mock_snow_cli, sample_profile):
        """Test that prompt has all required sections."""
        pass

    def test_build_analysis_prompt_includes_patterns(self, mock_snow_cli):
        """Test that detected patterns are included in prompt."""
        pass

    # Response parsing
    def test_parse_valid_json_response(self, mock_snow_cli):
        """Test parsing of well-formed JSON response."""
        pass

    def test_parse_json_in_markdown_code_block(self, mock_snow_cli):
        """Test handling of JSON wrapped in ```json ... ``` blocks."""
        pass

    def test_parse_malformed_json_falls_back(self, mock_snow_cli):
        """Test fallback when JSON is malformed."""
        pass

    # PII detection
    def test_pii_detection_email(self, mock_snow_cli):
        """Test PII detection for email columns (95%+ accuracy)."""
        pass

    def test_pii_detection_phone(self, mock_snow_cli):
        """Test PII detection for phone columns."""
        pass

    def test_pii_detection_ssn(self, mock_snow_cli):
        """Test PII detection for SSN/sensitive columns."""
        pass

    # Confidence scoring
    def test_confidence_level_high(self, mock_snow_cli):
        """Test confidence level calculation (80-100% = HIGH)."""
        pass

    def test_confidence_level_medium(self, mock_snow_cli):
        """Test confidence level calculation (60-79% = MEDIUM)."""
        pass

    def test_confidence_level_low(self, mock_snow_cli):
        """Test confidence level calculation (<60% = LOW)."""
        pass

    # Error handling
    def test_cortex_timeout_handled_gracefully(self, mock_snow_cli):
        """Test handling of Cortex Complete timeout."""
        pass
```

**Estimated LOC**: 500-600 (tests)

---

## Milestone 3: RelationshipDiscoverer Implementation

### File: `src/snowcli_tools/discovery/relationship_discoverer.py`

**Purpose**: Discover foreign key relationships using multiple strategies

**Key Requirements**:
- ✅ 75%+ precision (acceptable for 2-strategy approach)
- ✅ Strategy 1: Name pattern matching (60% accuracy, fast)
- ✅ Strategy 2: Value overlap analysis (80% accuracy, medium)
- ❌ Strategy 3: Query history (EXCLUDED - requires audit logs → v1.11.0)

**Implementation**:

```python
"""
RelationshipDiscoverer - Multi-strategy foreign key detection.

Discovers foreign key relationships that aren't explicitly defined using:
1. Name pattern matching (customer_id → CUSTOMERS table)
2. Value overlap analysis (check if values exist in target column)
"""

from typing import Optional, List
from dataclasses import dataclass

from snowcli_tools.discovery.models import Relationship
from snowcli_tools.snow_cli import SnowCLI


@dataclass
class RelationshipCandidate:
    """Candidate relationship before confidence scoring."""
    from_table: str
    from_column: str
    to_table: str
    to_column: str
    strategy: str
    raw_confidence: float
    evidence: list[str]


class RelationshipDiscoverer:
    """Discover foreign key relationships using multiple strategies."""

    def __init__(
        self,
        snow_cli: SnowCLI,
        database: str,
        schema: str,
        min_confidence: float = 0.7,
        sample_size: int = 1000
    ):
        """
        Initialize RelationshipDiscoverer.

        Args:
            snow_cli: SnowCLI instance
            database: Database to search for relationships
            schema: Schema to search for relationships
            min_confidence: Minimum confidence threshold (default: 0.7)
            sample_size: Sample size for value overlap analysis (default: 1000)
        """
        self.snow_cli = snow_cli
        self.database = database
        self.schema = schema
        self.min_confidence = min_confidence
        self.sample_size = sample_size

    def discover_relationships(
        self,
        table_name: str,
        column_name: str
    ) -> List[Relationship]:
        """
        Discover foreign key relationships for a column.

        Args:
            table_name: Source table name
            column_name: Source column name

        Returns:
            List of discovered Relationship objects
        """
        candidates = []

        # Strategy 1: Name pattern matching
        name_candidates = self._discover_by_name_patterns(table_name, column_name)
        candidates.extend(name_candidates)

        # Strategy 2: Value overlap analysis
        overlap_candidates = self._discover_by_value_overlap(table_name, column_name)
        candidates.extend(overlap_candidates)

        # Deduplicate and score
        relationships = self._deduplicate_and_score(candidates)

        # Filter by confidence threshold
        return [r for r in relationships if r.confidence >= self.min_confidence]

    def _discover_by_name_patterns(
        self,
        table_name: str,
        column_name: str
    ) -> List[RelationshipCandidate]:
        """
        Strategy 1: Name pattern matching.

        Patterns:
        - customer_id → CUSTOMERS.id
        - order_id → ORDERS.id
        - user_uuid → USERS.uuid
        """
        candidates = []

        # Common patterns
        patterns = [
            (r"(.+)_id$", "id"),       # customer_id → id
            (r"(.+)_uuid$", "uuid"),   # user_uuid → uuid
            (r"(.+)_key$", "key"),     # order_key → key
            (r"fk_(.+)$", "id"),       # fk_customer → id
        ]

        import re
        for pattern, target_col in patterns:
            match = re.match(pattern, column_name.lower())
            if match:
                # Extract table name from column
                potential_table = match.group(1).upper()

                # Check if table exists
                if self._table_exists(potential_table):
                    candidates.append(RelationshipCandidate(
                        from_table=table_name,
                        from_column=column_name,
                        to_table=potential_table,
                        to_column=target_col,
                        strategy="name_pattern",
                        raw_confidence=0.6,  # 60% for name matching
                        evidence=[f"Column name matches pattern: {column_name} → {potential_table}.{target_col}"]
                    ))

        return candidates

    def _discover_by_value_overlap(
        self,
        table_name: str,
        column_name: str
    ) -> List[RelationshipCandidate]:
        """
        Strategy 2: Value overlap analysis.

        Check if values in column A exist in column B of other tables.
        """
        candidates = []

        # Get sample values from source column
        source_sql = f"""
            SELECT DISTINCT "{column_name}"
            FROM "{self.database}"."{self.schema}"."{table_name}"
            WHERE "{column_name}" IS NOT NULL
            LIMIT {self.sample_size}
        """
        source_values = self.snow_cli.execute_query(source_sql)

        if not source_values:
            return candidates

        # Get all potential target columns (same data type)
        # TODO: Query INFORMATION_SCHEMA for matching columns

        # For each potential target, check overlap
        # ... (implementation continues)

        return candidates

    def _table_exists(self, table_name: str) -> bool:
        """Check if table exists in database/schema."""
        sql = f"""
            SELECT COUNT(*) as cnt
            FROM "{self.database}".INFORMATION_SCHEMA.TABLES
            WHERE TABLE_SCHEMA = '{self.schema}'
              AND TABLE_NAME = '{table_name}'
        """
        result = self.snow_cli.execute_query(sql)
        return int(result[0]["cnt"]) > 0

    def _deduplicate_and_score(
        self,
        candidates: List[RelationshipCandidate]
    ) -> List[Relationship]:
        """
        Deduplicate candidates and calculate combined confidence.

        If multiple strategies identify same relationship, boost confidence.
        """
        # Group by (from_table, from_column, to_table, to_column)
        grouped = {}
        for candidate in candidates:
            key = (candidate.from_table, candidate.from_column,
                   candidate.to_table, candidate.to_column)

            if key not in grouped:
                grouped[key] = []
            grouped[key].append(candidate)

        # Calculate combined confidence
        relationships = []
        for key, cands in grouped.items():
            # Combine evidence
            all_evidence = []
            for c in cands:
                all_evidence.extend(c.evidence)

            # Calculate confidence (higher if multiple strategies agree)
            if len(cands) == 1:
                confidence = cands[0].raw_confidence
                strategy = cands[0].strategy
            else:
                # Multiple strategies - boost confidence
                confidence = min(0.95, max(c.raw_confidence for c in cands) + 0.15)
                strategy = "combined"

            relationships.append(Relationship(
                from_table=key[0],
                from_column=key[1],
                to_table=key[2],
                to_column=key[3],
                confidence=confidence,
                evidence=all_evidence,
                strategy=strategy
            ))

        return relationships
```

**Estimated LOC**: 350-450

### Tests: `tests/discovery/test_relationships.py`

**Test Coverage** (15 tests):

```python
"""Tests for RelationshipDiscoverer."""

import pytest
from snowcli_tools.discovery.relationship_discoverer import RelationshipDiscoverer


class TestRelationshipDiscoverer:
    """Test suite for RelationshipDiscoverer."""

    # Name pattern matching
    def test_name_pattern_customer_id(self, mock_snow_cli):
        """Test customer_id → CUSTOMERS.id pattern."""
        pass

    def test_name_pattern_order_uuid(self, mock_snow_cli):
        """Test order_uuid → ORDERS.uuid pattern."""
        pass

    def test_name_pattern_fk_prefix(self, mock_snow_cli):
        """Test fk_customer → CUSTOMERS.id pattern."""
        pass

    def test_name_pattern_no_match(self, mock_snow_cli):
        """Test column with no pattern match returns empty."""
        pass

    # Value overlap analysis
    def test_value_overlap_high_confidence(self, mock_snow_cli):
        """Test value overlap >90% gives high confidence."""
        pass

    def test_value_overlap_medium_confidence(self, mock_snow_cli):
        """Test value overlap 70-90% gives medium confidence."""
        pass

    def test_value_overlap_low_confidence_filtered(self, mock_snow_cli):
        """Test value overlap <70% is filtered out."""
        pass

    # Combined strategies
    def test_combined_strategies_boost_confidence(self, mock_snow_cli):
        """Test that multiple strategies boost confidence."""
        pass

    def test_deduplication(self, mock_snow_cli):
        """Test that duplicate relationships are deduplicated."""
        pass

    # Confidence filtering
    def test_min_confidence_threshold(self, mock_snow_cli):
        """Test that relationships below threshold are filtered."""
        pass

    # Edge cases
    def test_no_relationships_found(self, mock_snow_cli):
        """Test table with no relationships returns empty list."""
        pass

    def test_self_referencing_relationship(self, mock_snow_cli):
        """Test detection of self-referencing FKs (parent_id → id)."""
        pass

    # Error handling
    def test_table_not_found(self, mock_snow_cli):
        """Test handling of non-existent source table."""
        pass

    def test_column_not_found(self, mock_snow_cli):
        """Test handling of non-existent column."""
        pass

    # Precision validation
    def test_relationship_precision_75_percent(self, mock_snow_cli):
        """Test that overall precision is 75%+ on test dataset."""
        pass
```

**Estimated LOC**: 500-600 (tests)

---

## Milestone 4: DocumentationGenerator & MCP Integration

### File: `src/snowcli_tools/discovery/documentation_generator.py`

**Purpose**: Generate human-readable and machine-readable outputs

**Key Requirements**:
- ✅ Markdown data dictionary (primary format for agents)
- ✅ JSON export (machine-readable)
- ✅ Mermaid ER diagrams
- ❌ YAML semantic models (EXCLUDED - defer to future)

**Implementation**:

```python
"""
DocumentationGenerator - Format discovery results.

Generates documentation in multiple formats:
- Markdown data dictionary (agent-optimized)
- JSON export (machine-readable)
- Mermaid ER diagrams (relationship visualization)
"""

from typing import Optional
from datetime import datetime

from snowcli_tools.discovery.models import (
    DiscoveryResult,
    TableProfile,
    LLMAnalysis,
    Relationship,
    OutputFormat
)


class DocumentationGenerator:
    """Generate documentation from discovery results."""

    def generate(
        self,
        result: DiscoveryResult,
        output_format: OutputFormat = OutputFormat.MARKDOWN
    ) -> str:
        """
        Generate documentation in specified format.

        Args:
            result: DiscoveryResult to document
            output_format: Format (markdown or json)

        Returns:
            Formatted documentation string
        """
        if output_format == OutputFormat.MARKDOWN:
            return self._generate_markdown(result)
        elif output_format == OutputFormat.JSON:
            return self._generate_json(result)
        else:
            raise ValueError(f"Unsupported output format: {output_format}")

    def _generate_markdown(self, result: DiscoveryResult) -> str:
        """
        Generate Markdown data dictionary.

        Optimized for agent consumption with clear structure.
        """
        profile = result.profile
        analysis = result.analysis
        relationships = result.relationships
        metadata = result.metadata

        lines = []

        # Title
        lines.append(f"# {profile.table_name} - Data Dictionary")
        lines.append("")

        # Metadata
        lines.append(f"**Database**: `{profile.database}.{profile.schema}.{profile.table_name}`")
        lines.append(f"**Rows**: {profile.row_count:,}")
        lines.append(f"**Last Modified**: {profile.last_ddl or 'Unknown'}")
        lines.append("")

        # Purpose (if analyzed)
        if analysis:
            lines.append("## Purpose")
            lines.append("")
            lines.append(analysis.suggested_description)
            lines.append("")
            lines.append(f"**Category**: {analysis.category.value}")
            lines.append(f"**Confidence**: {analysis.confidence*100:.0f}% ({metadata.confidence_level.value})")
            lines.append("")

        # Schema
        lines.append("## Schema")
        lines.append("")
        lines.append("| Column | Type | Purpose | Null% | Cardinality | PII |")
        lines.append("|--------|------|---------|-------|-------------|-----|")

        for col in profile.columns:
            col_purpose = ""
            is_pii = ""

            if analysis and col.name in analysis.column_meanings:
                meaning = analysis.column_meanings[col.name]
                col_purpose = meaning.purpose
                is_pii = "⚠️ Yes" if meaning.is_pii else "No"

            lines.append(
                f"| `{col.name}` | {col.data_type} | {col_purpose} | "
                f"{col.null_percentage:.1f}% | {col.cardinality:,} | {is_pii} |"
            )

        lines.append("")

        # Relationships (if discovered)
        if relationships:
            lines.append("## Relationships")
            lines.append("")

            # Mermaid diagram
            lines.append("```mermaid")
            lines.append("erDiagram")

            for rel in relationships:
                lines.append(
                    f"    {rel.from_table} ||--o{{ {rel.to_table} : \"{rel.from_column}\""
                )

            lines.append("```")
            lines.append("")

            # Table
            lines.append("| From Column | To Table | To Column | Confidence | Evidence |")
            lines.append("|-------------|----------|-----------|------------|----------|")

            for rel in relationships:
                evidence_str = rel.evidence[0] if rel.evidence else ""
                lines.append(
                    f"| `{rel.from_column}` | `{rel.to_table}` | `{rel.to_column}` | "
                    f"{rel.confidence*100:.0f}% | {evidence_str} |"
                )

            lines.append("")

        # Sample Data
        if profile.sample_rows:
            lines.append("## Sample Data")
            lines.append("")
            lines.append("```json")
            lines.append(json.dumps(profile.sample_rows[:3], indent=2, default=str))
            lines.append("```")
            lines.append("")

        # Discovery Metadata
        lines.append("## Discovery Metadata")
        lines.append("")
        lines.append(f"- **Discovery Mode**: {metadata.depth_mode.value}")
        lines.append(f"- **Execution Time**: {metadata.execution_time_ms}ms")
        lines.append(f"- **Estimated Cost**: ${metadata.estimated_cost_usd:.4f}")
        lines.append(f"- **Cache Hit**: {metadata.cache_hit}")
        lines.append(f"- **Discovery Version**: {metadata.discovery_version}")
        lines.append(f"- **Timestamp**: {metadata.timestamp}")

        return "\n".join(lines)

    def _generate_json(self, result: DiscoveryResult) -> str:
        """Generate JSON export."""
        return json.dumps(result.to_dict(), indent=2, default=str)
```

**Estimated LOC**: 250-350

### File: `src/snowcli_tools/mcp/tools/discover_table_purpose.py`

**Purpose**: MCP tool implementation

**Implementation**:

```python
"""
MCP Tool: discover_table_purpose

AI-powered table discovery and documentation.
"""

from typing import Any, Optional
from mcp import types

from snowcli_tools.discovery import (
    TableProfiler,
    LLMAnalyzer,
    RelationshipDiscoverer,
    DocumentationGenerator
)
from snowcli_tools.discovery.models import (
    DepthMode,
    OutputFormat,
    CachePolicy,
    DiscoveryResult,
    DiscoveryMetadata,
    ConfidenceLevel
)


class DiscoverTablePurposeTool:
    """
    MCP Tool: discover_table_purpose

    Discover and document poorly-documented Snowflake tables using AI.
    """

    name = "discover_table_purpose"
    description = """Discover and document Snowflake tables using AI-powered analysis.

Automatically profiles tables, infers business purpose using Cortex Complete,
discovers relationships, and generates comprehensive documentation.

Use this when:
- Encountering unfamiliar or poorly-documented tables
- Need to understand table structure and purpose quickly
- Want to discover foreign key relationships
- Generating data catalog documentation

Supports batch discovery of multiple tables in parallel."""

    def __init__(self, snow_cli):
        """Initialize tool with SnowCLI instance."""
        self.snow_cli = snow_cli

    @property
    def input_schema(self) -> dict[str, Any]:
        """MCP input schema."""
        return {
            "type": "object",
            "properties": {
                "table_name": {
                    "type": ["string", "array"],
                    "description": "Table name(s) to discover. Can be single string or array for batch discovery.",
                    "items": {"type": "string"}
                },
                "database": {
                    "type": "string",
                    "description": "Database name (optional, uses session default)"
                },
                "schema": {
                    "type": "string",
                    "description": "Schema name (optional, uses session default)"
                },
                "depth": {
                    "type": "string",
                    "enum": ["quick", "standard", "deep"],
                    "default": "standard",
                    "description": """Discovery depth:
- quick: SQL profiling only (5s, $0.01)
- standard: + AI analysis (15s, $0.05) [DEFAULT]
- deep: + Relationship discovery (25s, $0.08)"""
                },
                "output_format": {
                    "type": "string",
                    "enum": ["markdown", "json"],
                    "default": "markdown",
                    "description": "Output format (markdown or json)"
                },
                "cache_policy": {
                    "type": "string",
                    "enum": ["always", "if_fresh", "never"],
                    "default": "if_fresh",
                    "description": """Cache behavior:
- always: Use cache if available
- if_fresh: Use cache if table unchanged (LAST_DDL check) [DEFAULT]
- never: Always re-discover"""
                },
                "timeout_seconds": {
                    "type": "integer",
                    "default": 60,
                    "description": "Maximum execution time in seconds"
                }
            },
            "required": ["table_name"]
        }

    async def execute(self, arguments: dict[str, Any]) -> list[types.TextContent]:
        """
        Execute discovery.

        Returns discovery results as formatted documentation.
        """
        table_name = arguments["table_name"]
        database = arguments.get("database")
        schema = arguments.get("schema")
        depth = DepthMode(arguments.get("depth", "standard"))
        output_format = OutputFormat(arguments.get("output_format", "markdown"))
        cache_policy = CachePolicy(arguments.get("cache_policy", "if_fresh"))
        timeout = arguments.get("timeout_seconds", 60)

        # Handle batch discovery
        if isinstance(table_name, list):
            results = await self._batch_discover(
                table_name, database, schema, depth, output_format, cache_policy, timeout
            )
            return results
        else:
            result = await self._discover_single(
                table_name, database, schema, depth, output_format, cache_policy, timeout
            )
            return [result]

    async def _discover_single(
        self,
        table_name: str,
        database: Optional[str],
        schema: Optional[str],
        depth: DepthMode,
        output_format: OutputFormat,
        cache_policy: CachePolicy,
        timeout: int
    ) -> types.TextContent:
        """Discover single table."""
        import time
        start_time = time.time()

        # Check cache (if enabled)
        if cache_policy != CachePolicy.NEVER:
            cached = self._check_cache(table_name, database, schema, cache_policy)
            if cached:
                return types.TextContent(
                    type="text",
                    text=cached
                )

        # Initialize components
        profiler = TableProfiler(self.snow_cli)

        # Profile table (always)
        profile = profiler.profile_table(table_name, database, schema)

        # Analyze with LLM (if depth >= standard)
        analysis = None
        if depth in [DepthMode.STANDARD, DepthMode.DEEP]:
            analyzer = LLMAnalyzer(self.snow_cli)
            analysis = analyzer.analyze_table(profile)

        # Discover relationships (if depth = deep)
        relationships = []
        if depth == DepthMode.DEEP:
            discoverer = RelationshipDiscoverer(
                self.snow_cli,
                database or profile.database,
                schema or profile.schema
            )
            for col in profile.columns:
                col_rels = discoverer.discover_relationships(profile.table_name, col.name)
                relationships.extend(col_rels)

        # Generate documentation
        generator = DocumentationGenerator()

        # Build metadata
        end_time = time.time()
        execution_ms = int((end_time - start_time) * 1000)

        # Estimate cost
        cost = self._estimate_cost(depth, len(profile.columns))

        # Determine confidence level
        confidence_level = ConfidenceLevel.LOW
        if analysis:
            if analysis.confidence >= 0.8:
                confidence_level = ConfidenceLevel.HIGH
            elif analysis.confidence >= 0.6:
                confidence_level = ConfidenceLevel.MEDIUM

        metadata = DiscoveryMetadata(
            execution_time_ms=execution_ms,
            estimated_cost_usd=cost,
            cache_hit=False,
            table_last_modified=profile.last_ddl or "",
            discovery_version="1.10.0",
            confidence_level=confidence_level,
            depth_mode=depth,
            timestamp=datetime.now().isoformat()
        )

        # Build result
        result = DiscoveryResult(
            profile=profile,
            analysis=analysis,
            relationships=relationships,
            documentation="",
            metadata=metadata
        )

        # Generate documentation
        documentation = generator.generate(result, output_format)

        # Cache result
        self._cache_result(table_name, database, schema, documentation, metadata)

        return types.TextContent(
            type="text",
            text=documentation
        )

    async def _batch_discover(
        self,
        table_names: list[str],
        database: Optional[str],
        schema: Optional[str],
        depth: DepthMode,
        output_format: OutputFormat,
        cache_policy: CachePolicy,
        timeout: int
    ) -> list[types.TextContent]:
        """
        Batch discovery of multiple tables.

        Processes tables in parallel with error handling.
        """
        # TODO: Implement parallel processing with asyncio.gather
        results = []
        for table_name in table_names:
            try:
                result = await self._discover_single(
                    table_name, database, schema, depth, output_format, cache_policy, timeout
                )
                results.append(result)
            except Exception as e:
                # Include error in results (don't fail entire batch)
                error_content = types.TextContent(
                    type="text",
                    text=f"Error discovering {table_name}: {str(e)}"
                )
                results.append(error_content)

        return results

    def _estimate_cost(self, depth: DepthMode, column_count: int) -> float:
        """Estimate discovery cost in USD."""
        if depth == DepthMode.QUICK:
            return 0.01
        elif depth == DepthMode.STANDARD:
            return 0.05
        else:  # DEEP
            return 0.08

    def _check_cache(
        self,
        table_name: str,
        database: Optional[str],
        schema: Optional[str],
        cache_policy: CachePolicy
    ) -> Optional[str]:
        """Check discovery cache."""
        # TODO: Implement cache checking logic
        return None

    def _cache_result(
        self,
        table_name: str,
        database: Optional[str],
        schema: Optional[str],
        documentation: str,
        metadata: DiscoveryMetadata
    ):
        """Cache discovery result."""
        # TODO: Implement caching logic
        pass
```

**Estimated LOC**: 200-300

### Tests: Multiple test files

**Test Coverage** (33 tests total across files)

---

## Milestone 5: Enhanced Features

### Batch Discovery Implementation

**File**: Extend `discover_table_purpose.py`

**Features**:
- Accept `table_name: str | list[str]`
- Parallel processing with `asyncio.gather`
- Error handling (don't fail entire batch)
- Return `list[DiscoveryResult]`

**Estimated LOC**: ~150-200

### Cache Policy Implementation

**Features**:
- `CachePolicy.ALWAYS`: Use cache if available
- `CachePolicy.IF_FRESH`: Check LAST_DDL
- `CachePolicy.NEVER`: Always re-discover

**Cache Storage**:
```
.discovery_cache/
├── {database}/
│   └── {schema}/
│       └── {table}_{last_ddl_hash}.json
```

**Estimated LOC**: ~100-150

### Timeout Handling

**Implementation**:
- Use `asyncio.wait_for()` for timeout control
- Graceful degradation (return partial results)

**Estimated LOC**: ~50-80

### Confidence Interpretation

**Implementation**:
- Map confidence scores to levels (high/medium/low)
- Include interpretation in metadata
- Add helper method: `interpret_confidence(score: float) -> ConfidenceLevel`

**Estimated LOC**: ~30-50

---

## Milestone 6: Validation & Launch

### Test Strategy

**Total Tests**: 93
- TableProfiler: 20 tests
- LLMAnalyzer: 15 tests
- RelationshipDiscoverer: 15 tests
- DocumentationGenerator: 10 tests
- MCP Tool: 20 tests
- Batch Discovery: 8 tests
- Integration: 5 tests

**Coverage Target**: 90%+

### Accuracy Validation

**Test Dataset**: 20 diverse tables
- 5 dimension tables
- 5 fact tables
- 5 event logs
- 5 reference data tables

**Metrics**:
- [ ] Table purpose: 75%+ accuracy
- [ ] Column purpose: 70%+ useful
- [ ] Relationships: 75%+ precision
- [ ] PII detection: 95%+ recall

### Performance Benchmarks

- [ ] Quick mode: <5s (p95)
- [ ] Standard mode: <15s (p95)
- [ ] Deep mode: <25s (p95)

### Documentation

- [ ] README.md update
- [ ] CHANGELOG.md entry
- [ ] API documentation
- [ ] Usage examples
- [ ] Migration guide

### Version Bump

**File**: `pyproject.toml`

```toml
[project]
name = "snowcli-tools"
version = "1.10.0"  # Changed from 1.9.0
description = "... includes AI-powered Discovery Assistant for table profiling and documentation"
```

---

## Implementation Checklist

### Pre-Implementation
- [x] Create v1.10.0 branch
- [x] Review implementation spec
- [ ] Set up test environment
- [ ] Configure Snowflake test account

### Milestone 1: TableProfiler
- [ ] Implement `models.py` (data models)
- [ ] Implement `profiler.py` (TableProfiler)
- [ ] Fix SQL CROSS JOIN bug
- [ ] Add identifier quoting
- [ ] Implement pattern detection
- [ ] Write 20 tests
- [ ] Validate <5s for 1M rows

### Milestone 2: LLMAnalyzer
- [ ] Implement `llm_analyzer.py`
- [ ] Build Cortex Complete integration
- [ ] Implement prompt engineering
- [ ] Add JSON parsing with fallback
- [ ] Implement PII detection
- [ ] Write 15 tests
- [ ] Validate 75%+ accuracy

### Milestone 3: RelationshipDiscoverer
- [ ] Implement `relationship_discoverer.py`
- [ ] Add Strategy 1 (name patterns)
- [ ] Add Strategy 2 (value overlap)
- [ ] Implement confidence scoring
- [ ] Add deduplication logic
- [ ] Write 15 tests
- [ ] Validate 75%+ precision

### Milestone 4: Documentation & MCP
- [ ] Implement `documentation_generator.py`
- [ ] Add Markdown formatter
- [ ] Add JSON formatter
- [ ] Add Mermaid diagrams
- [ ] Implement `discover_table_purpose.py` (MCP tool)
- [ ] Register tool in MCP server
- [ ] Write 30 tests (20 MCP + 10 docs)

### Milestone 5: Enhanced Features
- [ ] Implement batch discovery
- [ ] Implement cache policy
- [ ] Implement timeout handling
- [ ] Implement confidence interpretation
- [ ] Add response metadata
- [ ] Write 13 tests

### Milestone 6: Validation & Launch
- [ ] Run all 93 tests (90%+ coverage)
- [ ] Accuracy validation (20-table test set)
- [ ] Performance benchmarks
- [ ] Security review
- [ ] Documentation complete
- [ ] Version bump to 1.10.0
- [ ] Create CHANGELOG entry
- [ ] Merge to main
- [ ] Create release tag
- [ ] Publish announcement

---

## Estimated Totals

**Code**:
- Production: 2,000-2,680 LOC
- Tests: 2,600-3,200 LOC
- Total: 4,600-5,880 LOC

**Timeline**: Milestone-based (2-3 weeks estimated)

**Quality Target**: 7.5/10+ (maintain v1.9.0 quality)

**Test Coverage**: 90%+

---

## Critical Success Factors

1. ✅ **Fix SQL CROSS JOIN bug** (Milestone 1)
2. ✅ **Add identifier quoting** (Milestone 1)
3. ✅ **75%+ accuracy validation** (Milestone 6)
4. ✅ **95%+ PII detection** (Milestone 2)
5. ✅ **Performance targets met** (Milestone 6)
6. ✅ **Code quality 7.5/10+** (All milestones)
7. ✅ **Zero breaking changes** (All milestones)

---

## Next Steps

1. **Commit this spec** to the repository
2. **Begin Milestone 1** (TableProfiler implementation)
3. **Daily standup**: Review progress, adjust estimates
4. **Weekly checkpoint**: Validate milestones complete

---

**Status**: ✅ READY FOR IMPLEMENTATION

**Branch**: `v1.10.0_discovery_assistant`

**Contact**: Evan Kim (@evandekim)
